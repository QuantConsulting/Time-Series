{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":18599,"databundleVersionId":1236839,"sourceType":"competition"},{"sourceId":996339,"sourceType":"datasetVersion","datasetId":546263},{"sourceId":2430966,"sourceType":"datasetVersion","datasetId":1092930},{"sourceId":4194327,"sourceType":"datasetVersion","datasetId":2473516}],"dockerImageVersionId":30236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transfer Learning for time series\n\nhttps://www.youtube.com/watch?v=izcmIemT-rU&t=1s","metadata":{"_kg_hide-input":false,"_kg_hide-output":true}},{"cell_type":"code","source":"!pip install darts[all]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-09-16T08:28:50.530381Z","iopub.execute_input":"2022-09-16T08:28:50.530688Z","iopub.status.idle":"2022-09-16T08:29:04.960662Z","shell.execute_reply.started":"2022-09-16T08:28:50.530624Z","shell.execute_reply":"2022-09-16T08:29:04.959514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from darts.dataprocessing.transformers import Scaler\nfrom darts import TimeSeries\nfrom darts.models import NaiveSeasonal, ARIMA, ExponentialSmoothing, NBEATSModel\nfrom darts.metrics import smape, mase, mape, rmse\n\nimport tqdm.notebook as tq\n\nimport seaborn as sns\n\nimport os\nimport time\nimport random\nimport pandas as pd\nimport numpy as np\n\nimport torch\nfrom torch import nn\n\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM,GRU,SimpleRNN, RNN, Input, Bidirectional\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\nfrom sklearn.model_selection import GroupKFold\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom datetime import timedelta\n\nfrom sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n\nfrom typing import List, Tuple, Dict\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight') \n\nimport warnings\nwarnings.simplefilter(action='ignore', category= FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:54:30.786485Z","iopub.execute_input":"2022-09-16T08:54:30.787052Z","iopub.status.idle":"2022-09-16T08:54:30.9075Z","shell.execute_reply.started":"2022-09-16T08:54:30.786944Z","shell.execute_reply":"2022-09-16T08:54:30.905076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    seed = 42    \n    img_dim1 = 20\n    img_dim2 = 10\n    horizon = 12\n    min_size = 48\n    start_point = '2014-01-01'\n    which_art = 'FOODS_3_819'\n    cutoff_point = '2016-05-08'\n    lookback = 120\n    lookahead = 14\n    nepochs = 10\n    bsize = 64\n    \n# adjust the parameters for displayed figures    \nplt.rcParams.update({'figure.figsize': (CFG.img_dim1,CFG.img_dim2)})   ","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:29:13.717556Z","iopub.execute_input":"2022-09-16T08:29:13.718769Z","iopub.status.idle":"2022-09-16T08:29:13.724996Z","shell.execute_reply.started":"2022-09-16T08:29:13.718712Z","shell.execute_reply":"2022-09-16T08:29:13.723992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function definitions\ndef plot_series(df, xname):\n    xd = df.loc[df.airlineid == xname ]\n    airname = xd['Description'].iat[0]\n    xd[['data_dte', 'Total']].set_index('data_dte').plot(title = airname, xlabel = '')\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:29:13.726465Z","iopub.execute_input":"2022-09-16T08:29:13.727074Z","iopub.status.idle":"2022-09-16T08:29:13.738106Z","shell.execute_reply.started":"2022-09-16T08:29:13.727037Z","shell.execute_reply":"2022-09-16T08:29:13.737105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-groundwork\"></a>\n# Groundwork\n\n\nOur reference series will be the data from the U.S. International Air Passenger and Freight Statistics Report on air traffic to and from the US: https://www.kaggle.com/datasets/parulpandey/us-international-air-traffic-data","metadata":{}},{"cell_type":"code","source":"xdat = pd.read_csv('../input/us-international-air-traffic-data/International_Report_Passengers.csv',\n                 usecols = ['data_dte', 'airlineid', 'Total']\n                  )\nxdat['data_dte'] = pd.to_datetime(xdat['data_dte'])\nxdat.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-09-23T07:32:52.85728Z","iopub.execute_input":"2022-09-23T07:32:52.858362Z","iopub.status.idle":"2022-09-23T07:32:54.26105Z","shell.execute_reply.started":"2022-09-23T07:32:52.858212Z","shell.execute_reply":"2022-09-23T07:32:54.259819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will aggregate to total passenger count per day\nxmat = xdat.groupby(['data_dte', 'airlineid'])['Total'].sum().reset_index()\nxmat['data_dte'] = pd.to_datetime(xmat['data_dte'])\nxmat.sort_values(by = 'data_dte', inplace = True)\n\n\n# map the identifiers to proper names\nxdict = pd.read_csv('../input/airline-codes-mapping/L_AIRLINE_ID.csv')\nxmat = pd.merge(left = xmat, right = xdict, left_on = 'airlineid', right_on = 'Code')\nxmat.drop('Code', axis = 1, inplace = True)\nxmat.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-09-23T07:33:00.124188Z","iopub.execute_input":"2022-09-23T07:33:00.124633Z","iopub.status.idle":"2022-09-23T07:33:00.300801Z","shell.execute_reply.started":"2022-09-23T07:33:00.124598Z","shell.execute_reply":"2022-09-23T07:33:00.299071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list of names \nnames_list = xmat.airlineid.unique()","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:29:15.021726Z","iopub.execute_input":"2022-09-16T08:29:15.022097Z","iopub.status.idle":"2022-09-16T08:29:15.030502Z","shell.execute_reply.started":"2022-09-16T08:29:15.022061Z","shell.execute_reply":"2022-09-16T08:29:15.029535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_series(xmat, names_list[0])","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:29:15.032065Z","iopub.execute_input":"2022-09-16T08:29:15.032487Z","iopub.status.idle":"2022-09-16T08:29:15.474295Z","shell.execute_reply.started":"2022-09-16T08:29:15.032451Z","shell.execute_reply":"2022-09-16T08:29:15.473383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_series(xmat, names_list[1])","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:29:15.475352Z","iopub.execute_input":"2022-09-16T08:29:15.475677Z","iopub.status.idle":"2022-09-16T08:29:15.903111Z","shell.execute_reply.started":"2022-09-16T08:29:15.475637Z","shell.execute_reply":"2022-09-16T08:29:15.902238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_series(xmat, names_list[31])","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:29:15.904594Z","iopub.execute_input":"2022-09-16T08:29:15.905581Z","iopub.status.idle":"2022-09-16T08:29:16.364089Z","shell.execute_reply.started":"2022-09-16T08:29:15.905543Z","shell.execute_reply":"2022-09-16T08:29:16.363205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see that the series exhibit quite different dynamics (including number of observations). Let's prepare the our time series with a clean train / test split (for a refresher about *proper* way to do cross-validation for time series, please check out [Episode 10](https://www.kaggle.com/code/konradb/ts-10-validation-methods-for-time-series)). \n\n\nThe approach we follow in this section is inspired by the AMLD workshop on transfer and meta learning https://appliedmldays.org/events/amld-epfl-2022/workshops/forecasting-meta-learning\n","metadata":{}},{"cell_type":"code","source":"\nall_series = []\nall_names = []\n\nfor (ii, name) in enumerate(names_list):\n    xd = xmat.loc[xmat.airlineid == name]\n    \n    # for speed sake we will downsample to monthly freq\n    x2 = xd[['data_dte', 'Total']].set_index('data_dte').resample('1M').sum().clip(lower = 1)\n    xseries = TimeSeries.from_dataframe(x2, freq = 'M')\n    \n    if len(xseries) > CFG.min_size:\n        \n        all_series.append(xseries)\n        all_names.append(name)\n\n        ","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:29:16.365443Z","iopub.execute_input":"2022-09-16T08:29:16.366395Z","iopub.status.idle":"2022-09-16T08:29:21.290776Z","shell.execute_reply.started":"2022-09-16T08:29:16.366354Z","shell.execute_reply":"2022-09-16T08:29:21.289786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train / test split\n\nair_train = [s[: - CFG.horizon] for s in all_series]\nair_test = [s[ - CFG.horizon:] for s in all_series]","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:29:21.29242Z","iopub.execute_input":"2022-09-16T08:29:21.293246Z","iopub.status.idle":"2022-09-16T08:29:23.155113Z","shell.execute_reply.started":"2022-09-16T08:29:21.293205Z","shell.execute_reply":"2022-09-16T08:29:23.154208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normalize the data \nprint('scaling series...')\nscaler_air = Scaler(scaler=MaxAbsScaler())\nair_train_scaled: List[TimeSeries] = scaler_air.fit_transform(air_train)\nair_test_scaled: List[TimeSeries] = scaler_air.transform(air_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:29:23.156696Z","iopub.execute_input":"2022-09-16T08:29:23.157083Z","iopub.status.idle":"2022-09-16T08:29:25.450233Z","shell.execute_reply.started":"2022-09-16T08:29:23.157045Z","shell.execute_reply":"2022-09-16T08:29:25.449295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper functions from the official Darts tutorial\n\n# evaluate a set of forecasts for multiple series (in darts format)    \ndef eval_forecasts(pred_series: List[TimeSeries], \n                   test_series: List[TimeSeries]) -> List[float]:\n  \n    print('computing sMAPEs...')\n    smapes = smape(test_series, pred_series)\n    mean, std = np.round(np.mean(smapes),4), np.round(np.std(smapes),4)\n    print('Avg sMAPE: %.3f +- %.3f' % (mean, std))\n    plt.figure(figsize = (CFG.img_dim1,CFG.img_dim2), dpi=144)\n    plt.hist(smapes, bins=50)\n    plt.ylabel('Count')\n    plt.xlabel('sMAPE')\n    plt.show()\n    plt.close()\n    return smapes\n\n\ndef eval_local_model(train_series: List[TimeSeries], \n                     test_series: List[TimeSeries], \n                     horizon,\n                     model_cls, \n                     **kwargs) -> Tuple[List[float], float]:\n    preds = []\n    start_time = time.time()\n    for series in tq.tqdm(train_series):\n        model = model_cls(**kwargs)\n        model.fit(series)\n        pred = model.predict(n=horizon)\n        preds.append(pred)\n    elapsed_time = time.time() - start_time\n    \n    smapes = eval_forecasts(preds, test_series)\n    return smapes, elapsed_time","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:29:25.45473Z","iopub.execute_input":"2022-09-16T08:29:25.455013Z","iopub.status.idle":"2022-09-16T08:29:25.464621Z","shell.execute_reply.started":"2022-09-16T08:29:25.454986Z","shell.execute_reply":"2022-09-16T08:29:25.463517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try a simple baseline: repeating the annual seasonal pattern","metadata":{}},{"cell_type":"code","source":"ns_last_smapes, ns_last_elapsed_time  = eval_local_model(air_train_scaled, air_test_scaled, \n                                                                                 CFG.horizon, NaiveSeasonal, K= 12)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:29:25.466166Z","iopub.execute_input":"2022-09-16T08:29:25.466594Z","iopub.status.idle":"2022-09-16T08:29:30.079939Z","shell.execute_reply.started":"2022-09-16T08:29:25.466558Z","shell.execute_reply":"2022-09-16T08:29:30.07899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What about something a little more bespoke?","metadata":{}},{"cell_type":"code","source":"import warnings\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\nwarnings.simplefilter('ignore', ConvergenceWarning)\nwarnings.simplefilter('ignore', UserWarning)\n\n\narima_smapes, arima_elapsed_time = eval_local_model(air_train_scaled, air_test_scaled, CFG.horizon, \n                                                                                 ARIMA, p=12, d=1, q=1)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:29:30.081564Z","iopub.execute_input":"2022-09-16T08:29:30.081993Z","iopub.status.idle":"2022-09-16T08:29:30.088327Z","shell.execute_reply.started":"2022-09-16T08:29:30.081951Z","shell.execute_reply":"2022-09-16T08:29:30.08645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es_smapes, es_elapsed_time = eval_local_model(air_train, air_test,  CFG.horizon, \n                                              ExponentialSmoothing)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:29:30.090153Z","iopub.execute_input":"2022-09-16T08:29:30.090609Z","iopub.status.idle":"2022-09-16T08:30:06.867038Z","shell.execute_reply.started":"2022-09-16T08:29:30.09057Z","shell.execute_reply":"2022-09-16T08:30:06.866071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the individual series tend to be rather short, we focus on vintage models here: the results above give us an indication what to expect; beating `NaiveSeasonal` is not trivial, apparently. \n\n\nLet's start exploiting the joint structure in the data, starting with fitting a single model for all series together - effectively first step towards transfer learning.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-joint\"></a>\n# Joint model\n\nFor our joint model, we will use `NBEATS`, which you might remember from [Episode 9](https://www.kaggle.com/code/konradb/ts-9-hybrid-methods): it's a deep learning architecture, built around something of a hierarchical aggregation of MLP.","metadata":{}},{"cell_type":"code","source":"from darts.utils.losses import SmapeLoss\n\n# Slicing hyper-params:\nIN_LEN = 24\nOUT_LEN = 12\n\n# Architecture hyper-params:\nNUM_STACKS = 18\nNUM_BLOCKS = 3\nNUM_LAYERS = 3\nLAYER_WIDTH = 180\nCOEFFS_DIM = 6\nLOSS_FN = SmapeLoss()\n\n# Training settings:\nLR = 5e-4\nBATCH_SIZE = 1024\nNUM_EPOCHS = 5\n","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:30:06.868814Z","iopub.execute_input":"2022-09-16T08:30:06.86963Z","iopub.status.idle":"2022-09-16T08:30:06.877253Z","shell.execute_reply.started":"2022-09-16T08:30:06.869582Z","shell.execute_reply":"2022-09-16T08:30:06.876377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\nstart_time = time.time()\nnbeats_model_air = NBEATSModel(input_chunk_length=IN_LEN,\n                               output_chunk_length=OUT_LEN, \n                               batch_size=BATCH_SIZE,\n                               nr_epochs_val_period=1,\n                               num_stacks=NUM_STACKS,\n                               num_blocks=NUM_BLOCKS,\n                               num_layers=NUM_LAYERS,\n                               layer_widths=LAYER_WIDTH,\n                               expansion_coefficient_dim=COEFFS_DIM,\n                               loss_fn=LOSS_FN,\n                               optimizer_kwargs={'lr': LR},\n                               pl_trainer_kwargs={\"enable_progress_bar\": True, \n                                                  \"accelerator\": \"gpu\",\n                                                  \"gpus\": -1,\n                                                  \"auto_select_gpus\": True},\n                               )\n\nnbeats_model_air.fit(air_train_scaled, num_loader_workers=1, epochs=NUM_EPOCHS)\n\nnb_preds = nbeats_model_air.predict(series = air_train_scaled, n = CFG.horizon)\nnbeats_elapsed_time = time.time() - start_time\nnbeats_smapes = eval_forecasts(nb_preds, air_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:30:06.87861Z","iopub.execute_input":"2022-09-16T08:30:06.879366Z","iopub.status.idle":"2022-09-16T08:32:28.178816Z","shell.execute_reply.started":"2022-09-16T08:30:06.879327Z","shell.execute_reply":"2022-09-16T08:32:28.177669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smapes_boxplot(method_to_smapes: Dict[str, List[float]], title: str):\n    method_names = []\n    smapes = []\n    for curr_method_name, curr_smapes in method_to_smapes.items():        \n        method_names += [curr_method_name] * len(curr_smapes)\n        smapes += curr_smapes\n    smapes_df = pd.DataFrame({'Method': method_names, 'sMAPE': smapes})\n    plt.figure(figsize=(CFG.img_dim1,CFG.img_dim2), dpi=144)\n    ax = sns.boxplot(x=\"Method\", y=\"sMAPE\", data=smapes_df)\n    ax.grid(False)\n    # Display median score on each box\n    medians = smapes_df.groupby(['Method'])['sMAPE'].median().round(decimals=2)\n    vertical_offset = smapes_df['sMAPE'].median() * 0.1\n    for xtick, name in enumerate(method_to_smapes.keys()):\n        \n        ax.text(xtick, medians[name] + vertical_offset, medians[name], \n                  horizontalalignment='center', size='x-small', color='w', weight='semibold')\n        plt.xticks(rotation=90) \n        plt.title(title) \n        plt.show()\n        plt.close()\n\ndef elapsed_time_barplot(method_to_elapsed_times: Dict[str, float], title: str):\n    elapsed_times_df = pd.DataFrame({'Method': method_to_elapsed_times.keys(), 'Elapsed time [s]': method_to_elapsed_times.values()})\n    ax = plt.figure(figsize=(CFG.img_dim1,CFG.img_dim2), dpi=144)\n    sns.barplot(x=\"Method\", y=\"Elapsed time [s]\", data=elapsed_times_df)\n    plt.xticks(rotation=90) \n    plt.title(title) \n    plt.show()\n    plt.close()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-09-16T08:32:28.180877Z","iopub.execute_input":"2022-09-16T08:32:28.181301Z","iopub.status.idle":"2022-09-16T08:32:28.193182Z","shell.execute_reply.started":"2022-09-16T08:32:28.181259Z","shell.execute_reply":"2022-09-16T08:32:28.192244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smapes  = pd.DataFrame.from_dict( {'exp smoothing': es_smapes, 'naive': ns_last_smapes,\n                                       'ARIMA': arima_smapes,\n                                   'NBEATS': nbeats_smapes})\n\nsmapes.boxplot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"etimes = pd.DataFrame({'Model': ['exp smoothing', 'naive', 'ARIMA', 'NBEATS'],\n                        'Time': [es_elapsed_time, ns_last_elapsed_time, arima_elapsed_time,\n                                nbeats_elapsed_time]})\nbargraph = etimes.plot(kind = 'bar',x = 'Model', y = 'Time', color = ['b', 'g', 'r', 'c'])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:44:27.052032Z","iopub.execute_input":"2022-09-16T08:44:27.052425Z","iopub.status.idle":"2022-09-16T08:44:27.400851Z","shell.execute_reply.started":"2022-09-16T08:44:27.052392Z","shell.execute_reply":"2022-09-16T08:44:27.399808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A joint model can offer performance on part with the local ones, if not the best of the bunch - although it less dispersed in its results. Can we improve the situation with more data? \n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-tl1\"></a>\n# TL: pretraining a time series model\n\nCV models went wild with the ability to pretrain on large amounts of images (Imagenet, Instagram), similar pattern emerged with NLP;  a simple adaptation of this approach to time series involves using a large collection of time series to train our model and then using it for prediction $\\rightarrow$ the usual point about downstream task / labeling is moot here.","metadata":{}},{"cell_type":"code","source":"xdat_m4 = pd.read_csv('../input/m4-forecasting-competition-dataset/Monthly-train.csv')\nxdat_m4.drop('V1', axis = 1, inplace = True)\nxdat_m4.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-09-16T08:54:47.271544Z","iopub.execute_input":"2022-09-16T08:54:47.272034Z","iopub.status.idle":"2022-09-16T08:55:06.807201Z","shell.execute_reply.started":"2022-09-16T08:54:47.271994Z","shell.execute_reply":"2022-09-16T08:55:06.806244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We proceed in a similar fashion as with the \"baseline\" model above: ","metadata":{}},{"cell_type":"code","source":"# create a list of series\nm4_series = []\n\nfor ii in range(xdat_m4.shape[0]):\n    \n    # drop the missing values - this can be improved by only dropping contiguous blocks\n    xd = xdat_m4.iloc[ii].dropna()\n\n    xseries = TimeSeries.from_values(xd.values)\n    \n    if len(xseries) > 172:        \n        m4_series.append(xseries)\n\n# train / test split\n\nm4_train = [s[: - 72] for s in m4_series]\nm4_test = [s[ - 72:] for s in m4_series]\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normalize the data \nscaler_m4 = Scaler(scaler=MaxAbsScaler())\nm4_train_scaled: List[TimeSeries] = scaler_m4.fit_transform(m4_train)\nm4_test_scaled: List[TimeSeries] = scaler_m4.transform(m4_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from darts.utils.losses import SmapeLoss\n\n# Slicing hyper-params:\nIN_LEN = 24\nOUT_LEN = 12\n# Architecture hyper-params:\nNUM_STACKS = 18\nNUM_BLOCKS = 3\nNUM_LAYERS = 3\nLAYER_WIDTH = 180\nCOEFFS_DIM = 6\nLOSS_FN = SmapeLoss()\n# Training settings:\nLR = 5e-4\nBATCH_SIZE = 1024\nMAX_SAMPLES_PER_TS = 64\nNUM_EPOCHS = 5\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nnbeats_model_m4 = NBEATSModel(input_chunk_length=IN_LEN,\n                               output_chunk_length=OUT_LEN, \n                               batch_size=BATCH_SIZE,\n                               nr_epochs_val_period=1,\n                               num_stacks=NUM_STACKS,\n                               num_blocks=NUM_BLOCKS,\n                               num_layers=NUM_LAYERS,\n                               layer_widths=LAYER_WIDTH,\n                               expansion_coefficient_dim=COEFFS_DIM,\n                               loss_fn=LOSS_FN,\n                               optimizer_kwargs={'lr': LR},\n                               pl_trainer_kwargs={\"enable_progress_bar\": True, \n                                                  \"accelerator\": \"gpu\",\n                                                  \"gpus\": -1,\n                                                  \"auto_select_gpus\": True},\n                               )\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-17T18:40:05.598687Z","iopub.execute_input":"2022-09-17T18:40:05.599262Z","iopub.status.idle":"2022-09-17T18:40:05.686186Z","shell.execute_reply.started":"2022-09-17T18:40:05.59919Z","shell.execute_reply":"2022-09-17T18:40:05.685386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nbeats_model_m4.fit(m4_train_scaled,\n                    num_loader_workers=1,\n                    max_samples_per_ts=MAX_SAMPLES_PER_TS,\n                    epochs=NUM_EPOCHS, val_series = m4_test_scaled,\n                    verbose = True\n                   )\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\nnbm4_preds = nbeats_model_m4.predict(series = air_train_scaled, n = CFG.horizon)\nnbm4_elapsed_time = time.time() - start_time\nnbm4_smapes = eval_forecasts(nbm4_preds, air_test_scaled)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smapes  = pd.DataFrame.from_dict( {'exp smoothing': es_smapes, 'naive': ns_last_smapes,\n                                       'ARIMA': arima_smapes,   'NBEATS': nbeats_smapes,\n                                       'NBEATS-M4': nbm4_smapes\n                                    })\n\nsmapes.boxplot()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see from the graph above, NBEATS pretrained on M4 is comparable in performance with the individual models per series - **despite the fact it was not trained on any of the airline data**. What about execution time?","metadata":{}},{"cell_type":"code","source":"etimes = pd.DataFrame({'Model': ['exp smoothing', 'naive', 'ARIMA', 'NBEATS', 'NBEATS-M4'],\n                        'Time': [es_elapsed_time, ns_last_elapsed_time, arima_elapsed_time,\n                                nbeats_elapsed_time, nbm4_elapsed_time]})\nbargraph = etimes.plot(kind = 'bar',x = 'Model', y = 'Time', color = ['b', 'g', 'r', 'c', 'y'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not only does it achieve performance (which can undoubtedly be improved with longer training / hyperparameters tuning), but it's also blazing fast - we don't need to train anything specifically for this application. \n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-tl2\"></a>\n# TL: the tinkering \n\nIn the previous section we re-used the same architecture fitted on a dataset with the same frequency as the target one. While those contraints are not very harsh, let us see how we can become more flexible in our transfer learning adventure. We will use the data from the [M5 Competition](https://www.kaggle.com/c/m5-forecasting-accuracy) competition  and recycle code bits from [Episode 6](https://www.kaggle.com/code/konradb/ts-6-deep-learning-for-ts-rnn-and-friends/):","metadata":{}},{"cell_type":"code","source":"## prepare the data\ncalendar_df = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv', parse_dates=['date'])\ncalendar_df = calendar_df.loc[:, ['date', 'wm_yr_wk', 'd']]\ndf = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')\ndf = df.loc[df.item_id== CFG.which_art]\ndf_T = df.melt(id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])\ndf_T.drop(columns=['id'], inplace=True)\n\nsales_df = df_T.merge(calendar_df, left_on='variable', right_on='d', how='left')\nsales_df.rename(columns={'value': 'sales_qty'}, inplace=True)\ndf = sales_df.loc[sales_df.date >= CFG.start_point, ['date', 'store_id', 'sales_qty']]\ndf['state_id'] = df['store_id'].str[:2]\n\n# create the long format matrix: individual stores\ndf_ind = df.groupby(['date', 'store_id'])[['sales_qty']].sum()\ndf_ind.reset_index(inplace=True)\ndf_ind = df_ind.T.reset_index(drop=True).T\ndf_ind.columns = ['ds', 'unique_id', 'sales']\n\ndf_ind.sort_values(by = ['unique_id','ds'], inplace = True)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## split into training and holdout\nxdat = df_ind.pivot(index='ds', columns='unique_id', values='sales')\n\nxtrain, xtest = xdat.iloc[:-(2 * CFG.lookahead + CFG.lookback)], xdat.iloc[-(2 * CFG.lookahead + CFG.lookback):]\nx0, x1 = xtrain.iloc[:-(2 * CFG.lookahead + CFG.lookback)], xtrain.iloc[- (2 * CFG.lookahead + CFG.lookback):]\n\n# scale / normalize the columns\nxcols = xtrain.columns\n\nRS = RobustScaler()\nRS.fit(xtrain)\n\nxtrain = RS.fit_transform(xtrain.to_numpy())\nxtrain = pd.DataFrame(xtrain, columns= xcols)\n\nxtest = RS.transform(xtest.to_numpy())\nxtest = pd.DataFrame(xtest, columns= xcols)","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## format into LSTM-friendly (so that it also works with the M4 data format)\ndef create_dataset(dataset, look_back, look_ahead):\n    xdat, ydat = [], []\n    for f in dataset.columns:\n        for i in range(len(dataset) - look_back -look_ahead):\n            xdat.append(dataset.iloc[i:i+ look_back][f])\n            ydat.append(dataset.iloc[i+ look_back : i + look_back + look_ahead][f])\n    xdat, ydat = np.array(xdat), np.array(ydat).reshape(-1,look_ahead)\n    xdat = xdat.reshape(-1, look_back, 1)\n    return xdat.astype(np.float32), ydat.astype(np.float32)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x0, y0 = create_dataset(dataset = x0, look_back = CFG.lookback, look_ahead = CFG.lookahead)\nx1, y1 = create_dataset(dataset = x1, look_back = CFG.lookback, look_ahead = CFG.lookahead)\n\nxtrain, ytrain = create_dataset(dataset = xtrain, look_back = CFG.lookback, look_ahead = CFG.lookahead)\nxtest, ytest = create_dataset(dataset = xtest, look_back = CFG.lookback, look_ahead = CFG.lookahead)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Try a simple architecture for the model we will crash test across datasets:","metadata":{}},{"cell_type":"code","source":"def create_model(out_dim):    \n    \n    model=Sequential()\n    model.add(SimpleRNN(10,input_shape= [None,1], return_sequences = True))\n    model.add(SimpleRNN(10,input_shape= [None,1]))\n    model.add(Dense(out_dim))\n    \n    model.compile(loss='mean_squared_error',optimizer='adam')\n    return model\n    \nbase_model = create_model(out_dim = CFG.lookahead)","metadata":{"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stop = EarlyStopping(monitor = 'val_loss', min_delta = 0.001, \n                           patience = 5, mode = 'min', verbose = 1,\n                           restore_best_weights = True)\n\nbase_model.fit(x0, y0, validation_data=(x1, y1), \n                  epochs = CFG.nepochs, batch_size = CFG.bsize, callbacks=[ early_stop])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The main reason for this part is to get a feel how many epochs are necessary - seems like 10 are fine. Let's fit the model on the complete dataset now: ","metadata":{}},{"cell_type":"code","source":"# fit on complete training data\nbase_model = create_model(out_dim = CFG.lookahead)\nbase_model.fit(xtrain, ytrain,  epochs = CFG.nepochs, batch_size = CFG.bsize)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predict on the holdout set, with necessary reshaping (to ensure inverse normalization works):","metadata":{}},{"cell_type":"code","source":"# predict on holdout\nypred_base = base_model.predict(xtest)\n\nypred_base = np.reshape(ypred_base.T, (-1,10), order = 'F')\nypred_base = RS.inverse_transform(ypred_base)\n\nytest = np.reshape(ytest.T, (-1,10), order = 'F')\nytest = RS.inverse_transform(ytest)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error as mse\n\n\ndef my_rmse(x,y):\n    return(np.round( np.sqrt(mse(x,y)) ,4))\n\nbase_error = my_rmse(ytest, ypred_base)\nprint(base_error)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This gives us the base error rate. What if we train the model on the M4 dataset first (using daily series so we capture similar types of patterns)?","metadata":{}},{"cell_type":"code","source":"m4dat = pd.read_csv('../input/m4-forecasting-competition-dataset/Daily-train.csv', nrows= 500)\nm4dat.drop('V1', axis = 1, inplace = True)\n# there are more elegant ways, I know...\nm4dat.dropna(axis = 1, how = 'any', inplace = True)\nprint(m4dat.shape)\n\nxcols2 = m4dat.columns\n\nRS2 = RobustScaler()\nRS2.fit(m4dat)\n\nm4dat = RS2.fit_transform(m4dat.to_numpy())\nm4dat = pd.DataFrame(m4dat, columns= xcols2)\n\nm4_model = create_model(out_dim = CFG.lookahead)\n\nm4_x, m4_y = create_dataset(dataset = m4dat, look_back = CFG.lookback, look_ahead = CFG.lookahead)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m4_model.fit(m4_x, m4_y,  epochs = 5, batch_size = CFG.bsize)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict on holdout\nypred_m4 = m4_model.predict(xtest)\n\nypred_m4 = np.reshape(ypred_m4.T, (-1,10), order = 'F')\nypred_m4 = RS.inverse_transform(ypred_m4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrain_error = my_rmse(ytest, ypred_m4)\nprint(pretrain_error)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unlike the airline dataset case in the previous section, this is solidly worse than the \"baseline\" model - reasons can include architecture simplicity (we just don't learn enough with two small layers) or the fact that difference in domains does actually matter in this instance. Proper investigation is outside the scope of this notebook, so let's instead do something we have not tried and finetune the pretrained model (~ moving from zero- to few-shot learning).","metadata":{}},{"cell_type":"code","source":"m4_model.fit(xtrain, ytrain,\n                  epochs = 5, batch_size = CFG.bsize)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict on holdout\nypred_m4f = m4_model.predict(xtest)\n\nypred_m4f = np.reshape(ypred_m4f.T, (-1,10), order = 'F')\nypred_m4f = RS.inverse_transform(ypred_m4f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"finetune_error = my_rmse(ytest, ypred_m4f)\nprint(my_rmse(ytest, ypred_m4f))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('baseline: ' + str(base_error))\nprint('pretrain: ' + str(pretrain_error))\nprint('finetune: ' + str(finetune_error))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Long story short, pretraining on a large dataset helps for time series :-) ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-tl3\"></a>\n# TL: more tinkering \n\nAfter the encouraging results of the preceding section I started wondering: was it just a fluke? The simplest way to address such question is to try an break something - in this instance, I decided to use M4 data but from a different frequency. My intuition was that the model would try to capture different patterns. ","metadata":{}},{"cell_type":"code","source":"m4dat_m = pd.read_csv('../input/m4-forecasting-competition-dataset/Monthly-train.csv', nrows= 500)\nm4dat_m.drop('V1', axis = 1, inplace = True)\nm4dat_m.dropna(axis = 1, how = 'any', inplace = True)\nprint(m4dat_m.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xcols_m = m4dat_m.columns\n\nRS_m = RobustScaler()\nRS_m.fit(m4dat_m)\n\nm4dat_m = RS_m.fit_transform(m4dat_m.to_numpy())\nm4dat_m = pd.DataFrame(m4dat_m, columns= xcols_m)\n\nm4_model_m = create_model(out_dim = CFG.lookahead)\n\nm4_x_m, m4_y_m = create_dataset(dataset = m4dat_m, look_back = CFG.lookback, look_ahead = CFG.lookahead)\n\nm4_model_m.fit(m4_x_m, m4_y_m,  epochs = 5, batch_size = CFG.bsize)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict on holdout\nypred_m4_m = m4_model_m.predict(xtest)\n\nypred_m4_m = np.reshape(ypred_m4_m.T, (-1,10), order = 'F')\nypred_m4_m = RS.inverse_transform(ypred_m4_m)\n\npretrain_error_m = my_rmse(ytest, ypred_m4_m)\nprint(pretrain_error_m)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m4_model_m.fit(xtrain, ytrain,\n                  epochs = 5, batch_size = CFG.bsize)\n\n# predict on holdout\nypred_m4_mf = m4_model_m.predict(xtest)\n\nypred_m4_mf = np.reshape(ypred_m4_mf.T, (-1,10), order = 'F')\nypred_m4_mf = RS.inverse_transform(ypred_m4_mf)\n\nfinetune_error_m = my_rmse(ytest, ypred_m4_mf)\nprint(finetune_error_m)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('baseline: ' + str(base_error))\nprint('pretrain: ' + str(pretrain_error))\nprint('pretrain_m: ' + str(pretrain_error_m))\nprint('finetune: ' + str(finetune_error))\nprint('finetune_m: ' + str(finetune_error_m))","metadata":{},"execution_count":null,"outputs":[]}]}